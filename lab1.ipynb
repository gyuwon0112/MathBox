{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNs9iaSnOoeH5/xTjOXPBbb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyuwon0112/MathBox/blob/main/lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1UdGLmQm6Jc"
      },
      "source": [
        "#[프로젝트 1] 파이썬으로 웹 스크래퍼 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IubEXSSnDVP"
      },
      "source": [
        "BeautifulSoup과 Selenium을 사용하여 빠르게 웹사이트를 스크랩하는 방법 실습하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc_g70FLnJ92"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2020111988 이규원\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJDISZWZnP8e"
      },
      "source": [
        "##서론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk-msBqJnL5m"
      },
      "source": [
        "이 프로젝트에서는 파이썬 라이브러리로 웹 스크래퍼를 어떻게 만드는지를 배운다. 웹 스크래핑은 매우 훌륭한 스킬이며 데이터와 관련된 직업군에서 요구되는 스킬\n",
        "이다. 또한 인터넷으로부터 이미 만들어져 있는 데이터세트를 다운로드하지 않고 자기 자신 만의 데이터를 모으기 위한 좋은 방법이 된다.\n",
        "이 프로젝트를 통해서 여러분은 다음에 대하여 익숙해질 것이다:\n",
        "* 웹 스크래핑이 무엇인가?\n",
        "* 웹 스크래핑을 위해서 왜 파이썬을 사용하는가?\n",
        "* 파이썬 라이브러리 -- BeautifulSoup과 Selenium\n",
        "* 웹사이트로 부터 데이터를 가져와서 dataframe에 저장\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5hf9VoanZz0"
      },
      "source": [
        "### 웹 스크래핑이 무엇인가?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-IIKtbnc3q"
      },
      "source": [
        "웹 스크래핑은 인터넷으로부터 데이터를 모으기 위한 자동화된 과정을 말한다.\n",
        "\n",
        "몇 시간동안의 수작업 없이 몇 줄의 코드를 간단히 실행시켜 모든 데이터를 가져올 수 있고 데이터 프레임에 저장할 수 있다.\n",
        "\n",
        "그 이후에는 이 데이터를 정렬하거나 원하는 정보를 쉽게 찾을 수 있다.\n",
        "\n",
        "이는 많은 시간과 노력을 줄여줄 수 있다. \n",
        "프로그래밍에 익숙치 않은 학생들도 코드를 통해 지겨운 작업을 자동화하는 것은 배워둘만한 스킬이라 할 수 있다.\n",
        "\n",
        "웹 스크래핑이 무엇인가?\n",
        "\n",
        "이것이 이 프로젝트에서 학습하고자 하는 것이며, 각 단계별로 코드와 함께 설명할 것입니다.\n",
        "\n",
        "미래에 다른 유사한 작업을 할 때에도 동일한 코드를 사용할 수 있을 것이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVDCHgugnfk7"
      },
      "source": [
        "### 데이터 스크래핑하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA0a_yTYno1G"
      },
      "source": [
        "이제 파이썬으로 웹 스크래핑하는 과정을 단계별로 설명할 것이다. 이 프로젝트에서는 구글의 Colaboratory를 사용하며, 다른 IDE를 사용해도 별 상관은 없다. 다만,\n",
        "시작하기 전에 Pandas 라이브러리를 설치해야 한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcd9YDvjVHMo"
      },
      "source": [
        "#!pip install beautifulsoup4\n",
        "#!pip install -U selenium\n",
        "#!apt install chromium-chromedriver\n",
        "#!pip install webdriver-manager\n",
        "!pip install kora -q\n",
        "\n",
        "from kora.selenium import wd\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from time import sleep\n",
        "import random\n",
        "import urllib.request as ur\n",
        "import pandas as pd\n",
        "\n",
        "Course = []\n",
        "Duration = []\n",
        "Start_Date = []\n",
        "Offered_By = []\n",
        "No_Of_Reviews = []\n",
        "rat = []\n",
        "t = []\n",
        "\n",
        "m = 1# page 수\n",
        "\n",
        "while m < (subject/15 + 1): # page까지의 정보 스크랩\n",
        "    if m == 1 : # 첫 page의 url\n",
        "        url = 'https://www.classcentral.com/subject/data-science' \n",
        "    else : # 두번째 page부터는 뒤를 수정해서 page만 넣어주면 됨\n",
        "        url = 'https://www.classcentral.com/subject/data-science?page=%d'%m\n",
        "    \n",
        "    #print(url)\n",
        "\n",
        "    html = bs(ur.urlopen(url).read(), \"html.parser\")\n",
        "    \n",
        "    \n",
        "    #wd.get(url)\n",
        "    #html = BeautifulSoup(wd.page_source)\n",
        "    #html = bs(ur.urlopen(url).read(), 'html.parser')\n",
        "\n",
        "\n",
        "    def find_1st(string, substring):\n",
        "        return string.find(substring, string.find(substring))\n",
        "        \n",
        "    def find_2nd(string, substring):\n",
        "        return string.find(substring, string.find(substring) + 1)\n",
        "\n",
        "    # Course\n",
        "    for i in html.findAll(\"h2\", {\"class\" : \"text-1 weight-semi line-tight margin-bottom-xxsmall\"}):\n",
        "        b = str(i)\n",
        "        #print(b[find_1st(b,'>')+1:find_2nd(b,'<')])\n",
        "        Course.append(b[find_1st(b,'>')+1:find_2nd(b,'<')])\n",
        "          \n",
        "\n",
        "    #print(Course)\n",
        "    course = []\n",
        "    for i in Course:\n",
        "        i = i.strip() # The strip() method removes any leading and trailing characters\n",
        "                    # (space is the default leading character to remove)\n",
        "        #print(i)\n",
        "        course.append(i)\n",
        "        \n",
        "\n",
        "    #print(course)\n",
        "\n",
        "    # Provider/Offered By:\n",
        "    for i in html.findAll('a',href=True, attrs={'class':'hover-underline color-charcoal text-3 margin-left-small line-tight'}):\n",
        "        b = str(i)\n",
        "        #print(b[find_1st(b,'>')+1:find_2nd(b,'<')])\n",
        "        Offered_By.append(b[find_1st(b,'>')+1:find_2nd(b,'<')])\n",
        "    \n",
        "    provider = []\n",
        "    for i in Offered_By:\n",
        "        i = i.strip()\n",
        "        provider.append(i)\n",
        "\n",
        "    #print(provider)\n",
        "\n",
        "    # Ratings\n",
        "    for d in html.findAll('span', attrs={'role':'img', 'class':'cmpt-rating-medium '}):\n",
        "        rat.append(d.get('aria-label'))\n",
        "\n",
        "    Rating = []\n",
        "    for i in rat:\n",
        "        i = i.strip()\n",
        "        #print(i)\n",
        "        Rating.append(i)\n",
        "\n",
        "\n",
        "\n",
        "    #print(Rating)\n",
        "\n",
        "    # Num of Reviews\n",
        "    for i in html.findAll(\"span\", attrs={'class':'text-3 color-gray margin-left-xxsmall'}):\n",
        "        b = str(i)\n",
        "        if i is not None :\n",
        "            No_Of_Reviews.append(i.text)\n",
        "        else:\n",
        "            No_Of_Reviews.append('-1')\n",
        "            \n",
        "        #print(b[find_1st(b,'>')+1:find_2nd(b,'<')])\n",
        "        #No_Of_Reviews.append(b[find_1st(b,'>')+1:find_2nd(b,'<')])\n",
        "    num_reviews = []\n",
        "    for i in No_Of_Reviews:\n",
        "        i = i.strip()\n",
        "        #print(i)\n",
        "        num_reviews.append(i)\n",
        "        \n",
        "    #print(num_reviews)\n",
        "\n",
        "    for d in html.findAll('li', attrs={'class':'nowrap padding-xsmall border-top border-gray-light row horz-align-left'}):\n",
        "        abc = d.find('span', attrs={'aria-label':'Workload and duration'})\n",
        "        if abc is not None:\n",
        "            #print(abc.text)\n",
        "            t.append(abc.text)\n",
        "            \n",
        "    duration = []\n",
        "    for i in t:\n",
        "        i = i.strip()\n",
        "        #print(i)\n",
        "        duration.append(i)\n",
        "\n",
        "    m = m + 1 # 페이지 넘기기\n",
        "\n",
        "    if(len(num_reviews)!=len(course)):\n",
        "        for p in range(len(course)-len(num_reviews)) :\n",
        "            num_reviews.append(-1)\n",
        "    \n",
        "    #print(len(course), len(Rating), len(num_reviews), len(provider), len(duration))\n",
        "    \n",
        "\n",
        "\n",
        "# 원하는 페이지까지 스크롤링한 후 출력한다\n",
        "dfDS = pd.DataFrame({'course':course, 'ratings':Rating,'No_of_Reviews':num_reviews,'provider':provider, 'Duration':duration})\n",
        "dfDS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpN_97QgQduJ"
      },
      "source": [
        "##3. 본 데이터로부터 얻을 수 있는 정보\n",
        "\n",
        "* 본 프로젝트에서는 훌륭한 데이터 과학 교육과정을 찾기 위해 [Class Central](https://www.classcentral.com/subject/data-science)이라고 불리는 홈페이지를  스크랩 한 것이다.\n",
        "\n",
        "* 이 홈페이지에는 '컴퓨터과학', '비지니스', '엔지니어', '교육' 등 다양한 토픽의 온라인 교육과정을 보여주고 있다. \n",
        "\n",
        "* 본 프로젝트에서는 '데이터과학'과 관련된 정보를 스크랩하였다.\n",
        "\n",
        "* 스크랩한 데이터에는 '강의명', '제공자', '평가', '리뷰', '기간'의 정보가 포함되어 있다.\n",
        "    * '리뷰' : 리뷰 수가 아닌 '-1'로 표기된 강의들은 리뷰가 등록되지 않은 신설 강의에 해당한다\n",
        "    * '기간' : 주당 지정된 시간만큼 학습 할 경우, 몇 주만에 끝낼 수 있는 지에 대한 정보를 제공한다."
      ]
    }
  ]
}